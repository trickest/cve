### [CVE-2024-42477](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2024-42477)
![](https://img.shields.io/static/v1?label=Product&message=llama.cpp&color=blue)
![](https://img.shields.io/static/v1?label=Version&message=%3C%20b3561%20&color=brightgreen)
![](https://img.shields.io/static/v1?label=Version&message=0%20&color=brightgreen)
![](https://img.shields.io/static/v1?label=Vulnerability&message=CWE-125%3A%20Out-of-bounds%20Read&color=brightgreen)

### Description

llama.cpp provides LLM inference in C/C++. The unsafe `type` member in the `rpc_tensor` structure can cause `global-buffer-overflow`. This vulnerability may lead to memory data leakage. The vulnerability is fixed in b3561.

### POC

#### Reference
- https://github.com/ggerganov/llama.cpp/commit/b72942fac998672a79a1ae3c03b340f7e629980b
- https://github.com/ggerganov/llama.cpp/security/advisories/GHSA-mqp6-7pv6-fqjf

#### Github
- https://github.com/7resp4ss/7resp4ss
- https://github.com/fkie-cad/nvd-json-data-feeds
- https://github.com/honysyang/eleaipoc

